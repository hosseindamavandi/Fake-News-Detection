{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "time: 1.11 s (started: 2023-07-25 13:11:26 +03:00)\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q ipython-autotime\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%timeit\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 125 ms (started: 2023-07-25 13:11:27 +03:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "from Utils import *\n",
    "from preprocess import *\n",
    "from models import ANN, CNN1D, BILSTM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44266, 101)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 140 ms (started: 2023-07-25 13:14:31 +03:00)\n"
     ]
    }
   ],
   "source": [
    "loaded_data = np.load(\"cleaned-dataset\\data_npy.npy\")\n",
    "loaded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 125 ms (started: 2023-07-25 13:11:27 +03:00)\n"
     ]
    }
   ],
   "source": [
    "loaded_data = np.load(\"cleaned-dataset\\data_npy.npy\")\n",
    "X_train, X_test, y_train, y_test = split_data(loaded_data, ratio=0.70, seed=42)\n",
    "\n",
    "#TODO #* SPLIT TO TRAIN, VAL, TEST to improve the model\n",
    "\n",
    "training_dataset = TextDataset(X_train, y_train)\n",
    "testing_dataset = TextDataset(X_test, y_test)\n",
    "train_loader = DataLoader(dataset=training_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(dataset=training_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 110 ms (started: 2023-07-25 13:11:27 +03:00)\n"
     ]
    }
   ],
   "source": [
    "def acc_func(y_true, y_pred):\n",
    "    y_pred = torch.round(y_pred)\n",
    "    return (y_true == y_pred).sum() / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 100])\n",
      "torch.Size([128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 110 ms (started: 2023-07-25 13:11:27 +03:00)\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "print(x.shape), print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 110 ms (started: 2023-07-25 13:11:27 +03:00)\n"
     ]
    }
   ],
   "source": [
    "from models import CNN1D\n",
    "model = CNN1D()\n",
    "\n",
    "# model(x.unsqueeze(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 110 ms (started: 2023-07-25 13:11:27 +03:00)\n"
     ]
    }
   ],
   "source": [
    "#* GLOABL VARIABLES\n",
    "EPOCHS = 150\n",
    "LEARNING_RATE = 3e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "#* LSTM \n",
    "INPUT_SIZE = train_loader.dataset[0][0].shape[0]\n",
    "HIDDEN_STATE = 64\n",
    "NUM_LAYERS = 4\n",
    "NUM_CLASSES = 1 #* binary classification\n",
    "\n",
    "model = CNN1D().to(DEVICE)\n",
    "# model = BILSTM(INPUT_SIZE, HIDDEN_STATE, NUM_LAYERS, NUM_CLASSES, bidirection=True).to(DEVICE)\n",
    "\n",
    "\n",
    "loss = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 157 ms (started: 2023-07-25 13:11:28 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# torch.save(ann.state_dict(), \"model.pth\")\n",
    "# ann_loaded = ANN().to(DEVICE)\n",
    "# ann_loaded.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 0.52517\n",
      "Epoch 0 | Train Acc: 81.89%\n",
      "Epoch 0 | Test Loss: 0.44652\n",
      "Epoch 0 | Test Acc: 91.42%\n",
      "Test loss decreased to 0.44652 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 1 | Train Loss: 0.41914\n",
      "Epoch 1 | Train Acc: 86.07%\n",
      "Epoch 1 | Test Loss: 0.27621\n",
      "Epoch 1 | Test Acc: 88.68%\n",
      "Test loss decreased from 0.44652 to 0.27621 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 2 | Train Loss: 0.27006\n",
      "Epoch 2 | Train Acc: 86.27%\n",
      "Epoch 2 | Test Loss: 0.20563\n",
      "Epoch 2 | Test Acc: 91.43%\n",
      "Test loss decreased from 0.27621 to 0.20563 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 3 | Train Loss: 0.24845\n",
      "Epoch 3 | Train Acc: 87.60%\n",
      "Epoch 3 | Test Loss: 0.19056\n",
      "Epoch 3 | Test Acc: 92.12%\n",
      "Test loss decreased from 0.20563 to 0.19056 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 4 | Train Loss: 0.23718\n",
      "Epoch 4 | Train Acc: 88.15%\n",
      "Epoch 4 | Test Loss: 0.18234\n",
      "Epoch 4 | Test Acc: 92.34%\n",
      "Test loss decreased from 0.19056 to 0.18234 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 5 | Train Loss: 0.23211\n",
      "Epoch 5 | Train Acc: 88.42%\n",
      "Epoch 5 | Test Loss: 0.17741\n",
      "Epoch 5 | Test Acc: 92.77%\n",
      "Test loss decreased from 0.18234 to 0.17741 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 6 | Train Loss: 0.22696\n",
      "Epoch 6 | Train Acc: 88.53%\n",
      "Epoch 6 | Test Loss: 0.17098\n",
      "Epoch 6 | Test Acc: 92.86%\n",
      "Test loss decreased from 0.17741 to 0.17098 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 7 | Train Loss: 0.22119\n",
      "Epoch 7 | Train Acc: 88.63%\n",
      "Epoch 7 | Test Loss: 0.16464\n",
      "Epoch 7 | Test Acc: 93.36%\n",
      "Test loss decreased from 0.17098 to 0.16464 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 8 | Train Loss: 0.21821\n",
      "Epoch 8 | Train Acc: 88.85%\n",
      "Epoch 8 | Test Loss: 0.15900\n",
      "Epoch 8 | Test Acc: 93.60%\n",
      "Test loss decreased from 0.16464 to 0.15900 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 9 | Train Loss: 0.21505\n",
      "Epoch 9 | Train Acc: 89.10%\n",
      "Epoch 9 | Test Loss: 0.15366\n",
      "Epoch 9 | Test Acc: 93.98%\n",
      "Test loss decreased from 0.15900 to 0.15366 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 10 | Train Loss: 0.20653\n",
      "Epoch 10 | Train Acc: 89.67%\n",
      "Epoch 10 | Test Loss: 0.14757\n",
      "Epoch 10 | Test Acc: 93.84%\n",
      "Test loss decreased from 0.15366 to 0.14757 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 11 | Train Loss: 0.20332\n",
      "Epoch 11 | Train Acc: 89.41%\n",
      "Epoch 11 | Test Loss: 0.14343\n",
      "Epoch 11 | Test Acc: 94.42%\n",
      "Test loss decreased from 0.14757 to 0.14343 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 12 | Train Loss: 0.19657\n",
      "Epoch 12 | Train Acc: 90.11%\n",
      "Epoch 12 | Test Loss: 0.13614\n",
      "Epoch 12 | Test Acc: 94.75%\n",
      "Test loss decreased from 0.14343 to 0.13614 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 13 | Train Loss: 0.19385\n",
      "Epoch 13 | Train Acc: 90.23%\n",
      "Epoch 13 | Test Loss: 0.13671\n",
      "Epoch 13 | Test Acc: 94.40%\n",
      "--------------------------------------------------\n",
      "Epoch 14 | Train Loss: 0.19330\n",
      "Epoch 14 | Train Acc: 90.17%\n",
      "Epoch 14 | Test Loss: 0.13373\n",
      "Epoch 14 | Test Acc: 94.82%\n",
      "Test loss decreased from 0.13614 to 0.13373 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 15 | Train Loss: 0.18924\n",
      "Epoch 15 | Train Acc: 90.33%\n",
      "Epoch 15 | Test Loss: 0.13158\n",
      "Epoch 15 | Test Acc: 94.55%\n",
      "Test loss decreased from 0.13373 to 0.13158 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 16 | Train Loss: 0.18762\n",
      "Epoch 16 | Train Acc: 90.23%\n",
      "Epoch 16 | Test Loss: 0.12423\n",
      "Epoch 16 | Test Acc: 95.30%\n",
      "Test loss decreased from 0.13158 to 0.12423 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 17 | Train Loss: 0.18500\n",
      "Epoch 17 | Train Acc: 90.44%\n",
      "Epoch 17 | Test Loss: 0.12543\n",
      "Epoch 17 | Test Acc: 94.93%\n",
      "--------------------------------------------------\n",
      "Epoch 18 | Train Loss: 0.18390\n",
      "Epoch 18 | Train Acc: 90.55%\n",
      "Epoch 18 | Test Loss: 0.11993\n",
      "Epoch 18 | Test Acc: 95.32%\n",
      "Test loss decreased from 0.12423 to 0.11993 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 19 | Train Loss: 0.17517\n",
      "Epoch 19 | Train Acc: 91.04%\n",
      "Epoch 19 | Test Loss: 0.11444\n",
      "Epoch 19 | Test Acc: 95.37%\n",
      "Test loss decreased from 0.11993 to 0.11444 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 20 | Train Loss: 0.17129\n",
      "Epoch 20 | Train Acc: 91.07%\n",
      "Epoch 20 | Test Loss: 0.10802\n",
      "Epoch 20 | Test Acc: 95.93%\n",
      "Test loss decreased from 0.11444 to 0.10802 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 21 | Train Loss: 0.17451\n",
      "Epoch 21 | Train Acc: 90.81%\n",
      "Epoch 21 | Test Loss: 0.11560\n",
      "Epoch 21 | Test Acc: 95.26%\n",
      "--------------------------------------------------\n",
      "Epoch 22 | Train Loss: 0.17125\n",
      "Epoch 22 | Train Acc: 91.10%\n",
      "Epoch 22 | Test Loss: 0.10882\n",
      "Epoch 22 | Test Acc: 95.86%\n",
      "--------------------------------------------------\n",
      "Epoch 23 | Train Loss: 0.16388\n",
      "Epoch 23 | Train Acc: 91.50%\n",
      "Epoch 23 | Test Loss: 0.10112\n",
      "Epoch 23 | Test Acc: 96.06%\n",
      "Test loss decreased from 0.10802 to 0.10112 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 24 | Train Loss: 0.16721\n",
      "Epoch 24 | Train Acc: 90.99%\n",
      "Epoch 24 | Test Loss: 0.09965\n",
      "Epoch 24 | Test Acc: 96.29%\n",
      "Test loss decreased from 0.10112 to 0.09965 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 25 | Train Loss: 0.16111\n",
      "Epoch 25 | Train Acc: 91.54%\n",
      "Epoch 25 | Test Loss: 0.09882\n",
      "Epoch 25 | Test Acc: 95.94%\n",
      "Test loss decreased from 0.09965 to 0.09882 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 26 | Train Loss: 0.15985\n",
      "Epoch 26 | Train Acc: 91.61%\n",
      "Epoch 26 | Test Loss: 0.09581\n",
      "Epoch 26 | Test Acc: 96.08%\n",
      "Test loss decreased from 0.09882 to 0.09581 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 27 | Train Loss: 0.15687\n",
      "Epoch 27 | Train Acc: 91.70%\n",
      "Epoch 27 | Test Loss: 0.09358\n",
      "Epoch 27 | Test Acc: 96.69%\n",
      "Test loss decreased from 0.09581 to 0.09358 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 28 | Train Loss: 0.15799\n",
      "Epoch 28 | Train Acc: 91.55%\n",
      "Epoch 28 | Test Loss: 0.09371\n",
      "Epoch 28 | Test Acc: 96.51%\n",
      "--------------------------------------------------\n",
      "Epoch 29 | Train Loss: 0.15424\n",
      "Epoch 29 | Train Acc: 91.75%\n",
      "Epoch 29 | Test Loss: 0.08609\n",
      "Epoch 29 | Test Acc: 96.85%\n",
      "Test loss decreased from 0.09358 to 0.08609 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 30 | Train Loss: 0.15021\n",
      "Epoch 30 | Train Acc: 92.00%\n",
      "Epoch 30 | Test Loss: 0.08881\n",
      "Epoch 30 | Test Acc: 96.26%\n",
      "--------------------------------------------------\n",
      "Epoch 31 | Train Loss: 0.14802\n",
      "Epoch 31 | Train Acc: 92.29%\n",
      "Epoch 31 | Test Loss: 0.09380\n",
      "Epoch 31 | Test Acc: 95.65%\n",
      "--------------------------------------------------\n",
      "Epoch 32 | Train Loss: 0.14959\n",
      "Epoch 32 | Train Acc: 91.93%\n",
      "Epoch 32 | Test Loss: 0.08135\n",
      "Epoch 32 | Test Acc: 96.75%\n",
      "Test loss decreased from 0.08609 to 0.08135 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 33 | Train Loss: 0.14820\n",
      "Epoch 33 | Train Acc: 91.81%\n",
      "Epoch 33 | Test Loss: 0.08303\n",
      "Epoch 33 | Test Acc: 96.52%\n",
      "--------------------------------------------------\n",
      "Epoch 34 | Train Loss: 0.14265\n",
      "Epoch 34 | Train Acc: 92.30%\n",
      "Epoch 34 | Test Loss: 0.07965\n",
      "Epoch 34 | Test Acc: 96.87%\n",
      "Test loss decreased from 0.08135 to 0.07965 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 35 | Train Loss: 0.14071\n",
      "Epoch 35 | Train Acc: 92.16%\n",
      "Epoch 35 | Test Loss: 0.07834\n",
      "Epoch 35 | Test Acc: 97.06%\n",
      "Test loss decreased from 0.07965 to 0.07834 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 36 | Train Loss: 0.14125\n",
      "Epoch 36 | Train Acc: 92.12%\n",
      "Epoch 36 | Test Loss: 0.07243\n",
      "Epoch 36 | Test Acc: 97.38%\n",
      "Test loss decreased from 0.07834 to 0.07243 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 37 | Train Loss: 0.14143\n",
      "Epoch 37 | Train Acc: 92.16%\n",
      "Epoch 37 | Test Loss: 0.07889\n",
      "Epoch 37 | Test Acc: 96.67%\n",
      "--------------------------------------------------\n",
      "Epoch 38 | Train Loss: 0.14159\n",
      "Epoch 38 | Train Acc: 92.37%\n",
      "Epoch 38 | Test Loss: 0.07378\n",
      "Epoch 38 | Test Acc: 97.61%\n",
      "--------------------------------------------------\n",
      "Epoch 39 | Train Loss: 0.13316\n",
      "Epoch 39 | Train Acc: 92.78%\n",
      "Epoch 39 | Test Loss: 0.09128\n",
      "Epoch 39 | Test Acc: 96.94%\n",
      "--------------------------------------------------\n",
      "Epoch 40 | Train Loss: 0.13846\n",
      "Epoch 40 | Train Acc: 92.32%\n",
      "Epoch 40 | Test Loss: 0.06873\n",
      "Epoch 40 | Test Acc: 97.34%\n",
      "Test loss decreased from 0.07243 to 0.06873 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 41 | Train Loss: 0.13273\n",
      "Epoch 41 | Train Acc: 92.73%\n",
      "Epoch 41 | Test Loss: 0.06948\n",
      "Epoch 41 | Test Acc: 97.12%\n",
      "--------------------------------------------------\n",
      "Epoch 42 | Train Loss: 0.13091\n",
      "Epoch 42 | Train Acc: 92.63%\n",
      "Epoch 42 | Test Loss: 0.06040\n",
      "Epoch 42 | Test Acc: 97.55%\n",
      "Test loss decreased from 0.06873 to 0.06040 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 43 | Train Loss: 0.12391\n",
      "Epoch 43 | Train Acc: 93.13%\n",
      "Epoch 43 | Test Loss: 0.06182\n",
      "Epoch 43 | Test Acc: 97.57%\n",
      "--------------------------------------------------\n",
      "Epoch 44 | Train Loss: 0.12697\n",
      "Epoch 44 | Train Acc: 92.94%\n",
      "Epoch 44 | Test Loss: 0.06699\n",
      "Epoch 44 | Test Acc: 97.70%\n",
      "--------------------------------------------------\n",
      "Epoch 45 | Train Loss: 0.12908\n",
      "Epoch 45 | Train Acc: 92.84%\n",
      "Epoch 45 | Test Loss: 0.06557\n",
      "Epoch 45 | Test Acc: 97.14%\n",
      "--------------------------------------------------\n",
      "Epoch 46 | Train Loss: 0.12576\n",
      "Epoch 46 | Train Acc: 92.93%\n",
      "Epoch 46 | Test Loss: 0.05799\n",
      "Epoch 46 | Test Acc: 97.34%\n",
      "Test loss decreased from 0.06040 to 0.05799 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 47 | Train Loss: 0.12643\n",
      "Epoch 47 | Train Acc: 92.78%\n",
      "Epoch 47 | Test Loss: 0.06848\n",
      "Epoch 47 | Test Acc: 96.38%\n",
      "--------------------------------------------------\n",
      "Epoch 48 | Train Loss: 0.12610\n",
      "Epoch 48 | Train Acc: 92.78%\n",
      "Epoch 48 | Test Loss: 0.05364\n",
      "Epoch 48 | Test Acc: 97.86%\n",
      "Test loss decreased from 0.05799 to 0.05364 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 49 | Train Loss: 0.11782\n",
      "Epoch 49 | Train Acc: 93.24%\n",
      "Epoch 49 | Test Loss: 0.05336\n",
      "Epoch 49 | Test Acc: 98.07%\n",
      "Test loss decreased from 0.05364 to 0.05336 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 50 | Train Loss: 0.12265\n",
      "Epoch 50 | Train Acc: 92.94%\n",
      "Epoch 50 | Test Loss: 0.04832\n",
      "Epoch 50 | Test Acc: 97.85%\n",
      "Test loss decreased from 0.05336 to 0.04832 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 51 | Train Loss: 0.12041\n",
      "Epoch 51 | Train Acc: 93.10%\n",
      "Epoch 51 | Test Loss: 0.04820\n",
      "Epoch 51 | Test Acc: 98.23%\n",
      "Test loss decreased from 0.04832 to 0.04820 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 52 | Train Loss: 0.11655\n",
      "Epoch 52 | Train Acc: 93.35%\n",
      "Epoch 52 | Test Loss: 0.04804\n",
      "Epoch 52 | Test Acc: 98.17%\n",
      "Test loss decreased from 0.04820 to 0.04804 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 53 | Train Loss: 0.12551\n",
      "Epoch 53 | Train Acc: 92.98%\n",
      "Epoch 53 | Test Loss: 0.04481\n",
      "Epoch 53 | Test Acc: 98.25%\n",
      "Test loss decreased from 0.04804 to 0.04481 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 54 | Train Loss: 0.11840\n",
      "Epoch 54 | Train Acc: 93.21%\n",
      "Epoch 54 | Test Loss: 0.04879\n",
      "Epoch 54 | Test Acc: 98.01%\n",
      "--------------------------------------------------\n",
      "Epoch 55 | Train Loss: 0.11562\n",
      "Epoch 55 | Train Acc: 93.19%\n",
      "Epoch 55 | Test Loss: 0.04158\n",
      "Epoch 55 | Test Acc: 98.59%\n",
      "Test loss decreased from 0.04481 to 0.04158 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 56 | Train Loss: 0.11616\n",
      "Epoch 56 | Train Acc: 93.31%\n",
      "Epoch 56 | Test Loss: 0.04510\n",
      "Epoch 56 | Test Acc: 97.88%\n",
      "--------------------------------------------------\n",
      "Epoch 57 | Train Loss: 0.11129\n",
      "Epoch 57 | Train Acc: 93.54%\n",
      "Epoch 57 | Test Loss: 0.04354\n",
      "Epoch 57 | Test Acc: 98.39%\n",
      "--------------------------------------------------\n",
      "Epoch 58 | Train Loss: 0.11650\n",
      "Epoch 58 | Train Acc: 93.35%\n",
      "Epoch 58 | Test Loss: 0.06131\n",
      "Epoch 58 | Test Acc: 96.69%\n",
      "--------------------------------------------------\n",
      "Epoch 59 | Train Loss: 0.11224\n",
      "Epoch 59 | Train Acc: 93.39%\n",
      "Epoch 59 | Test Loss: 0.04954\n",
      "Epoch 59 | Test Acc: 97.73%\n",
      "--------------------------------------------------\n",
      "Epoch 60 | Train Loss: 0.10869\n",
      "Epoch 60 | Train Acc: 93.57%\n",
      "Epoch 60 | Test Loss: 0.04088\n",
      "Epoch 60 | Test Acc: 98.71%\n",
      "Test loss decreased from 0.04158 to 0.04088 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 61 | Train Loss: 0.11002\n",
      "Epoch 61 | Train Acc: 93.47%\n",
      "Epoch 61 | Test Loss: 0.03414\n",
      "Epoch 61 | Test Acc: 98.99%\n",
      "Test loss decreased from 0.04088 to 0.03414 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 62 | Train Loss: 0.10718\n",
      "Epoch 62 | Train Acc: 93.61%\n",
      "Epoch 62 | Test Loss: 0.04215\n",
      "Epoch 62 | Test Acc: 97.97%\n",
      "--------------------------------------------------\n",
      "Epoch 63 | Train Loss: 0.10537\n",
      "Epoch 63 | Train Acc: 93.65%\n",
      "Epoch 63 | Test Loss: 0.03881\n",
      "Epoch 63 | Test Acc: 98.70%\n",
      "--------------------------------------------------\n",
      "Epoch 64 | Train Loss: 0.11223\n",
      "Epoch 64 | Train Acc: 93.45%\n",
      "Epoch 64 | Test Loss: 0.03916\n",
      "Epoch 64 | Test Acc: 98.39%\n",
      "--------------------------------------------------\n",
      "Epoch 65 | Train Loss: 0.11164\n",
      "Epoch 65 | Train Acc: 93.36%\n",
      "Epoch 65 | Test Loss: 0.03177\n",
      "Epoch 65 | Test Acc: 98.63%\n",
      "Test loss decreased from 0.03414 to 0.03177 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 66 | Train Loss: 0.10478\n",
      "Epoch 66 | Train Acc: 93.73%\n",
      "Epoch 66 | Test Loss: 0.03126\n",
      "Epoch 66 | Test Acc: 98.65%\n",
      "Test loss decreased from 0.03177 to 0.03126 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 67 | Train Loss: 0.10637\n",
      "Epoch 67 | Train Acc: 93.72%\n",
      "Epoch 67 | Test Loss: 0.03138\n",
      "Epoch 67 | Test Acc: 98.98%\n",
      "--------------------------------------------------\n",
      "Epoch 68 | Train Loss: 0.09797\n",
      "Epoch 68 | Train Acc: 94.24%\n",
      "Epoch 68 | Test Loss: 0.03419\n",
      "Epoch 68 | Test Acc: 98.83%\n",
      "--------------------------------------------------\n",
      "Epoch 69 | Train Loss: 0.10379\n",
      "Epoch 69 | Train Acc: 93.76%\n",
      "Epoch 69 | Test Loss: 0.02784\n",
      "Epoch 69 | Test Acc: 98.70%\n",
      "Test loss decreased from 0.03126 to 0.02784 saving new best model\n",
      "--------------------------------------------------\n",
      "Epoch 70 | Train Loss: 0.10198\n",
      "Epoch 70 | Train Acc: 93.76%\n",
      "Epoch 70 | Test Loss: 0.03010\n",
      "Epoch 70 | Test Acc: 99.06%\n",
      "--------------------------------------------------\n",
      "Interrupted, returning saved history\n",
      "time: 2min 52s (started: 2023-07-25 13:11:28 +03:00)\n"
     ]
    }
   ],
   "source": [
    "history = Training(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    EPOCHS,\n",
    "    DEVICE,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    print_every=1,\n",
    "    load_saved_model=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(history)\n\u001b[0;32m      2\u001b[0m history\u001b[39m.\u001b[39mdropna(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m history\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    665\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    116\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    119\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\pandas\\core\\internals\\construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    664\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[0;32m    665\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 666\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    668\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[0;32m    669\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    670\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    671\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 203 ms (started: 2023-07-25 13:01:27 +03:00)\n"
     ]
    }
   ],
   "source": [
    "history = pd.DataFrame(history)\n",
    "history.dropna(inplace=True)\n",
    "history\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(10, 7))\n",
    "# plt.plot(history[\"accuracy\"], label=\"train_acc\")\n",
    "# # plt.plot(history[\"val_accuracy\"][:-1], label=\"val_acc\")\n",
    "\n",
    "# #* change the axix limit\n",
    "# plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 125 ms (started: 2023-07-25 12:51:53 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# loaded_ann = ANN().to(DEVICE)\n",
    "# loaded_ann.load_state_dict(torch.load(\"ANN_best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model loaded successfully!\n",
      "time: 875 ms (started: 2023-07-25 13:01:50 +03:00)\n"
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\AI projects\\\\Fake-News-Detection\\\\word2vec_model\\\\word2vec.model\"\n",
    "word2vec_model = Load_word2vec(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fake, with probability: 28.77%'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.75 s (started: 2023-07-25 13:01:52 +03:00)\n"
     ]
    }
   ],
   "source": [
    "from preprocess import *\n",
    "sample = \"The billionaire changed his profile picture to the new logo and added X.com to his Twitter bio.\"\n",
    "#0.01\n",
    "predict(sample, word2vec_model, model, DEVICE, prepare_for=model._get_name())\n",
    "# sample = preprocess_text(sample)\n",
    "# sample = vectorize_text(sample, word2vec_model)\n",
    "# sample = torch.tensor(sample).float().to(DEVICE).unsqueeze(0).unsqueeze(1)\n",
    "# print(sample.shape)\n",
    "# model.eval()\n",
    "# model(sample)\n",
    "# print(predict(sample, word2vec_model, loaded_ann, DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ann' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# Prediction\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m y_pred \u001b[39m=\u001b[39m ann(torch\u001b[39m.\u001b[39mtensor(X_test)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(DEVICE))\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Accuracy score\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel accuracy is \u001b[39m\u001b[39m\"\u001b[39m,accuracy_score(y_pred,y_test))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ann' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 250 ms (started: 2023-07-25 12:51:56 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# Importing metrics\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Prediction\n",
    "y_pred = ann(torch.tensor(X_test).float().to(DEVICE)).to(\"cpu\").detach().numpy().reshape(-1)\n",
    "\n",
    "\n",
    "# Accuracy score\n",
    "print(\"Model accuracy is \",accuracy_score(y_pred,y_test))\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_pred=y_pred,y_true=y_test)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(6,6))\n",
    "sns.heatmap(confusion_matrix,annot=True,fmt=\"0.1f\",linewidths=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
