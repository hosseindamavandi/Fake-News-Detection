# -*- coding: utf-8 -*-
"""visu.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YY8bV0uSJow0Ghie-yQY4BXWHe9no8Nb
"""

#some of  These codes are adapted from the "Project : FND" by "PRANEETH REDDY PESALA"
# Source: "https://www.kaggle.com/code/praneethreddyphn/project-fnd"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from wordcloud import WordCloud
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

true_data=pd.read_csv("")
fake_data=pd.read_csv("")

true_data.head()

fake_data.head()

true_data["label"]=0
fake_data["label"]=1

df=pd.concat([true_data,fake_data])

df.info()

df.head()

sns.set_style("ticks")
sns.countplot(df , x = "label")

plt.figure(figsize = (12,8))
sns.set(style = "dark",font_scale = 1.4)
chart = sns.countplot(x = "subject", hue = "label" , data = df)
chart.set_xticklabels(chart.get_xticklabels(),rotation=90)

df['text_len'] = df['text'].str.len()
plt.scatter(df[df['label'] == 1]['text_len'], df[df['label'] == 1]['label'], color='blue', label='Real')
plt.scatter(df[df['label'] == 0]['text_len'], df[df['label'] == 0]['label'], color='red', label='Fake')
plt.xlabel('Text Length')
plt.ylabel('Label')
plt.legend()

labels = ['Real', 'Fake']
sizes = df['label'].value_counts().values
colors = ['blue', 'red']
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')
plt.axis('equal')

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))
word=df[df["label"]==1]['text'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')
ax1.set_title('Original text')
word=df[df['label']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')
ax2.set_title('Fake text')
fig.suptitle('Average word length in each text')

stop_words = set(stopwords.words('english'))

# Create a Word Cloud
text = " ".join(df['text'].apply(lambda x: " ".join([word for word in x.lower().split() if word not in stop_words])).values.astype(str))
wordcloud = WordCloud(width=800, height=500, max_words=200, background_color='white').generate(text)
plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")

from sklearn.feature_extraction.text import CountVectorizer
def mcw(body, i, c):
    vector = CountVectorizer(ngram_range=(c, c)).fit(body)
    bwords = vector.transform(body)
    sum_words = bwords.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vector.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:i]

plt.figure(figsize = (16,9))
most_common_tri =mcw(df.text,10,3)
most_common_tri = dict(most_common_tri)
sns.barplot(x=list(most_common_tri.values()),y=list(most_common_tri.keys()))